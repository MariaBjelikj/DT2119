{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjelikj/DT2119/blob/master/Labs/Lab3/Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_300NuWNwpuZ",
        "colab_type": "code",
        "outputId": "4200b2f3-a216-447f-bf97-53a5cf489c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDs-WPLA6Fq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP6t6vObxeDP",
        "colab_type": "code",
        "outputId": "3c4bfc7f-2071-4948-a346-4995d15b14cb",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "\"\"\"\n",
        "Necessary files:\n",
        "- lab3_proto.py\n",
        "- lab3_tools.py\n",
        "- lab1_proto.py\n",
        "- lab1_tools.py\n",
        "- prondict.py\n",
        "\n",
        "If not uploaded to Google Drive:\n",
        "- lab2_models_all.npz\n",
        "- lab3_example.npz\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "if IN_COLAB:\n",
        "    !rm lab1_proto.py\n",
        "    !rm lab1_tools.py\n",
        "    !rm lab2_proto.py\n",
        "    !rm lab2_tools.py\n",
        "    !rm lab3_proto.py\n",
        "    !rm lab3_tools.py\n",
        "    !rm lab3_example.npz\n",
        "    !rm lab2_models_all.npz\n",
        "    !ls\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0a07703c-e0f9-4cdb-be0e-928065eb0719\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0a07703c-e0f9-4cdb-be0e-928065eb0719\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving lab1_proto.py to lab1_proto.py\n",
            "Saving lab1_tools.py to lab1_tools.py\n",
            "Saving lab2_proto.py to lab2_proto.py\n",
            "Saving lab2_tools.py to lab2_tools.py\n",
            "Saving lab3_proto.py to lab3_proto.py\n",
            "Saving lab3_tools.py to lab3_tools.py\n",
            "Saving prondict.py to prondict.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lab1_proto.py': b'# DT2119, Lab 1 Feature Extraction\\r\\nimport numpy as np\\r\\nimport matplotlib.pyplot as plt\\r\\nimport scipy.signal as ssi\\r\\nfrom scipy import fftpack\\r\\nfrom lab1_tools import trfbank\\r\\nfrom scipy.fftpack.realtransforms import dct\\r\\nfrom lab1_tools import *\\r\\nfrom scipy.spatial.distance import euclidean\\r\\nfrom scipy.spatial.distance import cdist\\r\\n\\r\\n# Function given by the exercise ----------------------------------\\r\\n\\r\\ndef mspec(samples, winlen=400, winshift=200, preempcoeff=0.97, nfft=512, samplingrate=20000):\\r\\n    \"\"\"Computes Mel Filterbank features.\\r\\n\\r\\n    Args:\\r\\n        samples: array of speech samples with shape (N,)\\r\\n        winlen: lenght of the analysis window\\r\\n        winshift: number of samples to shift the analysis window at every time step\\r\\n        preempcoeff: pre-emphasis coefficient\\r\\n        nfft: length of the Fast Fourier Transform (power of 2, >= winlen)\\r\\n        samplingrate: sampling rate of the original signal\\r\\n\\r\\n    Returns:\\r\\n        N x nfilters array with mel filterbank features (see trfbank for nfilters)\\r\\n    \"\"\"\\r\\n    frames = enframe(samples, winlen, winshift)\\r\\n    preemph = preemp(frames, preempcoeff)\\r\\n    windowed = windowing(preemph)\\r\\n    spec = powerSpectrum(windowed, nfft)\\r\\n    return logMelSpectrum(spec, samplingrate)\\r\\n\\r\\ndef mfcc(samples, winlen = 400, winshift = 200, preempcoeff=0.97, nfft=512, nceps=13, samplingrate=20000, liftercoeff=22):\\r\\n    \"\"\"Computes Mel Frequency Cepstrum Coefficients.\\r\\n\\r\\n    Args:\\r\\n        samples: array of speech samples with shape (N,)\\r\\n        winlen: lenght of the analysis window\\r\\n        winshift: number of samples to shift the analysis window at every time step\\r\\n        preempcoeff: pre-emphasis coefficient\\r\\n        nfft: length of the Fast Fourier Transform (power of 2, >= winlen)\\r\\n        nceps: number of cepstrum coefficients to compute\\r\\n        samplingrate: sampling rate of the original signal\\r\\n        liftercoeff: liftering coefficient used to equalise scale of MFCCs\\r\\n\\r\\n    Returns:\\r\\n        N x nceps array with lifetered MFCC coefficients\\r\\n    \"\"\"\\r\\n    mspecs = mspec(samples, winlen, winshift, preempcoeff, nfft, samplingrate)\\r\\n    ceps = cepstrum(mspecs, nceps)\\r\\n    return lifter(ceps, liftercoeff)\\r\\n\\r\\n# Functions to be implemented ----------------------------------\\r\\n\\r\\ndef enframe(samples, winlen, winshift):\\r\\n    \"\"\"\\r\\n    Slices the input samples into overlapping windows.\\r\\n\\r\\n    Args:\\r\\n        winlen: window length in samples.\\r\\n        winshift: shift of consecutive windows in samples\\r\\n    Returns:\\r\\n        numpy array [N x winlen], where N is the number of windows that fit\\r\\n        in the input signal\\r\\n    \"\"\"\\r\\n    \\r\\n    frames = np.array(samples[0:winlen]) # Start array here with the first frame\\r\\n                                         # and stack all the frames on top for \\r\\n                                         # each window shift\\r\\n    for i in range(winshift, len(samples) - winlen, winshift): \\r\\n        frames = np.vstack([frames, samples[i:i+winlen]]) # extract and stack frames\\r\\n        \\r\\n    return frames\\r\\n    \\r\\n    \\r\\ndef preemp(input, p=0.97):\\r\\n    \"\"\"\\r\\n    Pre-emphasis filter.\\r\\n\\r\\n    Args:\\r\\n        input: array of speech frames [N x M] where N is the number of frames and\\r\\n               M the samples per frame\\r\\n        p: preemhasis factor (defaults to the value specified in the exercise)\\r\\n\\r\\n    Output:\\r\\n        output: array of pre-emphasised speech samples\\r\\n    Note (you can use the function lfilter from scipy.signal)\\r\\n    \"\"\"\\r\\n    \\r\\n    # For the definition of the filter coefficients, check the documentation at\\r\\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.lfilter.html\\r\\n    # and slide 67 in lecture 02 (alpha = p = 0.97)\\r\\n    return ssi.lfilter([1, -p], [1], input)\\r\\n\\r\\n\\r\\ndef windowing(input):\\r\\n    \"\"\"\\r\\n    Applies hamming window to the input frames.\\r\\n\\r\\n    Args:\\r\\n        input: array of speech samples [N x M] where N is the number of frames and\\r\\n               M the samples per frame\\r\\n    Output:\\r\\n        array of windoed speech samples [N x M]\\r\\n    Note (you can use the function hamming from scipy.signal, include the sym=0 option\\r\\n    if you want to get the same results as in the example)\\r\\n    \"\"\"\\r\\n    \\r\\n    hamming_window = ssi.hamming(input.shape[1], sym=False)\\r\\n    \\r\\n    # Why we use hamming window at: \\r\\n    # https://stackoverflow.com/questions/5418951/what-is-the-hamming-window-for\\r\\n    return hamming_window * input\\r\\n\\r\\n\\r\\ndef powerSpectrum(input, nfft):\\r\\n    \"\"\"\\r\\n    Calculates the power spectrum of the input signal, that is the square of the modulus of the FFT\\r\\n\\r\\n    Args:\\r\\n        input: array of speech samples [N x M] where N is the number of frames and\\r\\n               M the samples per frame\\r\\n        nfft: length of the FFT\\r\\n    Output:\\r\\n        array of power spectra [N x nfft]\\r\\n    Note: you can use the function fft from scipy.fftpack\\r\\n    \"\"\"\\r\\n    \\r\\n    return np.power(np.abs(fftpack.fft(input, nfft)), 2)\\r\\n\\r\\ndef logMelSpectrum(input, samplingrate):\\r\\n    \"\"\"\\r\\n    Calculates the log output of a Mel filterbank when the input is the power spectrum\\r\\n\\r\\n    Args:\\r\\n        input: array of power spectrum coefficients [N x nfft] where N is the number of frames and\\r\\n               nfft the length of each spectrum\\r\\n        samplingrate: sampling rate of the original signal (used to calculate the filterbank shapes)\\r\\n    Output:\\r\\n        array of Mel filterbank log outputs [N x nmelfilters] where nmelfilters is the number\\r\\n        of filters in the filterbank\\r\\n    Note: use the trfbank function provided in lab1_tools.py to calculate the filterbank shapes and\\r\\n          nmelfilters\\r\\n    \"\"\"\\r\\n    \\r\\n    return np.log(input.dot(trfbank(samplingrate, input.shape[1]).T))\\r\\n\\r\\n\\r\\ndef cepstrum(input, nceps):\\r\\n    \"\"\"\\r\\n    Calulates Cepstral coefficients from mel spectrum applying Discrete Cosine Transform\\r\\n\\r\\n    Args:\\r\\n        input: array of log outputs of Mel scale filterbank [N x nmelfilters] where N is the\\r\\n               number of frames and nmelfilters the length of the filterbank\\r\\n        nceps: number of output cepstral coefficients\\r\\n    Output:\\r\\n        array of Cepstral coefficients [N x nceps]\\r\\n    Note: you can use the function dct from scipy.fftpack.realtransforms\\r\\n    \"\"\"\\r\\n\\r\\n    return dct(input)[:, :nceps]\\r\\n    \\r\\ndef dtw(x, y, dist, Comp_Dist = False):\\r\\n    \"\"\"Dynamic Time Warping.\\r\\n\\r\\n    Args:\\r\\n        x, y: arrays of size NxD and MxD respectively, where D is the dimensionality\\r\\n              and N, M are the respective lenghts of the sequences\\r\\n        dist: distance function (can be used in the code as dist(x[i], y[j]))\\r\\n\\r\\n    Outputs:\\r\\n        d: global distance between the sequences (scalar) normalized to len(x)+len(y)\\r\\n        LD: local distance between frames from x and y (NxM matrix)\\r\\n        AD: accumulated distance between frames of x and y (NxM matrix)\\r\\n        path: best path through AD\\r\\n\\r\\n    Note that you only need to define the first output for this exercise.\\r\\n    \"\"\"\\r\\n\\r\\n    LD = np.zeros((x.shape[0],y.shape[0]))\\r\\n    # LD = cdist(x,y,metric=\\'euclidean\\')\\r\\n    AD = np.copy(LD)\\r\\n    H, K = AD.shape\\r\\n    path = []\\r\\n    idx_arr = [[1, 0], [1, 1], [0, 1]]\\r\\n    prev_idx = np.zeros((2, H, K), dtype=int)\\r\\n    h_path = H - 1\\r\\n    k_path = K - 1\\r\\n    \\r\\n    for h in range(H):\\r\\n        for k in range(K):\\r\\n            LD[h, k] = dist(x[h],y[k])\\r\\n            prev_dist = [AD[h-1, k], AD[h-1, k-1], AD[h, k-1]]\\r\\n            prev_idx[:, h, k] = idx_arr[np.argmin(prev_dist)]\\r\\n            AD[h][k] = LD[h][k] + min(prev_dist)\\r\\n\\r\\n    d = AD[-1][-1]\\r\\n\\r\\n    if Comp_Dist:\\r\\n        return d\\r\\n\\r\\n    while (h_path>=0 and k_path>=0):\\r\\n        path.append([h_path, k_path])\\r\\n        h_path, k_path = np.array([h_path,k_path]) - prev_idx[:, h_path, k_path]\\r\\n\\r\\n    return d, LD, AD, np.asarray(path)\\r\\n\\r\\ndef compare(frames, example_frames):\\r\\n    \"\"\" Returns True if frames and example_frame are equal \"\"\"\\r\\n    \\r\\n    return np.isclose(frames, example_frames).all()\\r\\n\\r\\ndef getEuclidean(x, y):\\r\\n        return euclidean(x,y)\\r\\n',\n",
              " 'lab1_tools.py': b'import numpy as np\\r\\n# DT2119, Lab 1 Feature Extraction\\r\\n# - Functions given by the exercise -------------------------------------------- \\r\\n\\r\\ndef tidigit2labels(tidigitsarray):\\r\\n    \"\"\"\\r\\n    Return a list of labels including gender, speaker, digit and repetition information for each\\r\\n    utterance in tidigitsarray. Useful for plots.\\r\\n    \"\"\"\\r\\n    labels = []\\r\\n    nex = len(tidigitsarray)\\r\\n    for ex in range(nex):\\r\\n        labels.append(tidigitsarray[ex][\\'gender\\'] + \\'_\\' + \\r\\n                      tidigitsarray[ex][\\'speaker\\'] + \\'_\\' + \\r\\n                      tidigitsarray[ex][\\'digit\\'] + \\'_\\' + \\r\\n                      tidigitsarray[ex][\\'repetition\\'])\\r\\n    return labels\\r\\n\\r\\ndef dither(samples, level=1.0):\\r\\n    \"\"\"\\r\\n    Applies dithering to the samples. Adds Gaussian noise to the samples to avoid numerical\\r\\n        errors in the subsequent FFT calculations.\\r\\n\\r\\n        samples: array of speech samples\\r\\n        level: decides the amount of dithering (see code for details)\\r\\n\\r\\n    Returns:\\r\\n        array of dithered samples (same shape as samples)\\r\\n    \"\"\"\\r\\n    return samples + level*np.random.normal(0,1, samples.shape)\\r\\n    \\r\\n\\r\\ndef lifter(mfcc, lifter=22):\\r\\n    \"\"\"\\r\\n    Applies liftering to improve the relative range of MFCC coefficients.\\r\\n\\r\\n       mfcc: NxM matrix where N is the number of frames and M the number of MFCC coefficients\\r\\n       lifter: lifering coefficient\\r\\n\\r\\n    Returns:\\r\\n       NxM array with lifeterd coefficients\\r\\n    \"\"\"\\r\\n    nframes, nceps = mfcc.shape\\r\\n    cepwin = 1.0 + lifter/2.0 * np.sin(np.pi * np.arange(nceps) / lifter)\\r\\n    return np.multiply(mfcc, np.tile(cepwin, nframes).reshape((nframes,nceps)))\\r\\n\\r\\ndef hz2mel(f):\\r\\n    \"\"\"Convert an array of frequency in Hz into mel.\"\"\"\\r\\n    return 1127.01048 * np.log(f/700 +1)\\r\\n\\r\\ndef trfbank(fs, nfft, lowfreq=133.33, linsc=200/3., logsc=1.0711703, nlinfilt=13, nlogfilt=27, equalareas=False):\\r\\n    \"\"\"Compute triangular filterbank for MFCC computation.\\r\\n\\r\\n    Inputs:\\r\\n    fs:         sampling frequency (rate)\\r\\n    nfft:       length of the fft\\r\\n    lowfreq:    frequency of the lowest filter\\r\\n    linsc:      scale for the linear filters\\r\\n    logsc:      scale for the logaritmic filters\\r\\n    nlinfilt:   number of linear filters\\r\\n    nlogfilt:   number of log filters\\r\\n\\r\\n    Outputs:\\r\\n    res:  array with shape [N, nfft], with filter amplitudes for each column.\\r\\n            (N=nlinfilt+nlogfilt)\\r\\n    From scikits.talkbox\"\"\"\\r\\n    # Total number of filters\\r\\n    nfilt = nlinfilt + nlogfilt\\r\\n\\r\\n    #------------------------\\r\\n    # Compute the filter bank\\r\\n    #------------------------\\r\\n    # Compute start/middle/end points of the triangular filters in spectral\\r\\n    # domain\\r\\n    freqs = np.zeros(nfilt+2)\\r\\n    freqs[:nlinfilt] = lowfreq + np.arange(nlinfilt) * linsc\\r\\n    freqs[nlinfilt:] = freqs[nlinfilt-1] * logsc ** np.arange(1, nlogfilt + 3)\\r\\n    if equalareas:\\r\\n        heights = np.ones(nfilt)\\r\\n    else:\\r\\n        heights = 2./(freqs[2:] - freqs[0:-2])\\r\\n\\r\\n    # Compute filterbank coeff (in fft domain, in bins)\\r\\n    fbank = np.zeros((nfilt, nfft))\\r\\n    # FFT bins (in Hz)\\r\\n    nfreqs = np.arange(nfft) / (1. * nfft) * fs\\r\\n    for i in range(nfilt):\\r\\n        low = freqs[i]\\r\\n        cen = freqs[i+1]\\r\\n        hi = freqs[i+2]\\r\\n\\r\\n        lid = np.arange(np.floor(low * nfft / fs) + 1,\\r\\n                        np.floor(cen * nfft / fs) + 1, dtype=np.int)\\r\\n        lslope = heights[i] / (cen - low)\\r\\n        rid = np.arange(np.floor(cen * nfft / fs) + 1,\\r\\n                        np.floor(hi * nfft / fs) + 1, dtype=np.int)\\r\\n        rslope = heights[i] / (hi - cen)\\r\\n        fbank[i][lid] = lslope * (nfreqs[lid] - low)\\r\\n        fbank[i][rid] = rslope * (hi - nfreqs[rid])\\r\\n\\r\\n    return fbank\\r\\n',\n",
              " 'lab2_proto.py': b'import numpy as np\\r\\nfrom lab2_tools import *\\r\\n\\r\\ndef concatTwoHMMs(hmm1, hmm2):\\r\\n    \"\"\" Concatenates 2 HMM models\\r\\n\\r\\n    Args:\\r\\n       hmm1, hmm2: two dictionaries with the following keys:\\r\\n           name: phonetic or word symbol corresponding to the model\\r\\n           startprob: M+1 array with priori probability of state\\r\\n           transmat: (M+1)x(M+1) transition matrix\\r\\n           means: MxD array of mean vectors\\r\\n           covars: MxD array of variances\\r\\n\\r\\n    D is the dimension of the feature vectors\\r\\n    M is the number of emitting states in each HMM model (could be different for each)\\r\\n\\r\\n    Output\\r\\n       dictionary with the same keys as the input but concatenated models:\\r\\n          startprob: K+1 array with priori probability of state\\r\\n          transmat: (K+1)x(K+1) transition matrix\\r\\n             means: KxD array of mean vectors\\r\\n            covars: KxD array of variances\\r\\n\\r\\n    K is the sum of the number of emitting states from the input models\\r\\n   \\r\\n    Example:\\r\\n       twoHMMs = concatHMMs(phoneHMMs[\\'sil\\'], phoneHMMs[\\'ow\\'])\\r\\n\\r\\n    See also: the concatenating_hmms.pdf document in the lab package\\r\\n    \"\"\"\\r\\n    HMMs = {}\\r\\n    HMMs[\\'name\\'] = hmm1[\\'name\\'] + hmm2[\\'name\\']\\r\\n\\r\\n    # Concatenate start probabilities\\r\\n    HMMs[\\'startprob\\'] = np.hstack((hmm1[\\'startprob\\'][0:-1], np.multiply(hmm1[\\'startprob\\'][-1], hmm2[\\'startprob\\'])))\\r\\n    \\r\\n    # Concatenate transition matrices\\r\\n    temp1 = np.hstack((hmm1[\\'transmat\\'][0:-1, 0:-1], hmm1[\\'transmat\\'][0:-1, -1][np.newaxis].T.dot(hmm2[\\'startprob\\'][np.newaxis])))\\r\\n    temp2 = np.hstack((np.zeros((hmm2[\\'transmat\\'].shape[0], hmm1[\\'transmat\\'].shape[1] - 1)), hmm2[\\'transmat\\']))\\r\\n    HMMs[\\'transmat\\'] = np.vstack((temp1, temp2))\\r\\n    HMMs[\\'transmat\\'] = np.vstack((temp1, temp2))\\r\\n    \\r\\n    \\r\\n    # Concatenate means\\r\\n    HMMs[\\'means\\'] = np.vstack((hmm1[\\'means\\'], hmm2[\\'means\\']))\\r\\n    \\r\\n    # Concatenate covariances\\r\\n    HMMs[\\'covars\\'] = np.vstack((hmm1[\\'covars\\'], hmm2[\\'covars\\']))\\r\\n    \\r\\n    return HMMs\\r\\n\\r\\n\\r\\n# this is already implemented, but based on concat2HMMs() above\\r\\ndef concatHMMs(hmmmodels, namelist):\\r\\n    \"\"\" Concatenates HMM models in a left to right manner\\r\\n\\r\\n    Args:\\r\\n       hmmmodels: dictionary of models indexed by model name. \\r\\n       hmmmodels[name] is a dictionaries with the following keys:\\r\\n           name: phonetic or word symbol corresponding to the model\\r\\n           startprob: M+1 array with priori probability of state\\r\\n           transmat: (M+1)x(M+1) transition matrix\\r\\n           means: MxD array of mean vectors\\r\\n           covars: MxD array of variances\\r\\n       namelist: list of model names that we want to concatenate\\r\\n\\r\\n    D is the dimension of the feature vectors\\r\\n    M is the number of emitting states in each HMM model (could be\\r\\n      different in each model)\\r\\n\\r\\n    Output\\r\\n       combinedhmm: dictionary with the same keys as the input but\\r\\n                    combined models:\\r\\n         startprob: K+1 array with priori probability of state\\r\\n          transmat: (K+1)x(K+1) transition matrix\\r\\n             means: KxD array of mean vectors\\r\\n            covars: KxD array of variances\\r\\n\\r\\n    K is the sum of the number of emitting states from the input models\\r\\n\\r\\n    Example:\\r\\n       wordHMMs[\\'o\\'] = concatHMMs(phoneHMMs, [\\'sil\\', \\'ow\\', \\'sil\\'])\\r\\n    \"\"\"\\r\\n    concat = hmmmodels[namelist[0]]\\r\\n    for idx in range(1, len(namelist)):\\r\\n        concat = concatTwoHMMs(concat, hmmmodels[namelist[idx]])\\r\\n    return concat\\r\\n\\r\\n\\r\\ndef gmmloglik(log_emlik, weights):\\r\\n    \"\"\"Log Likelihood for a GMM model based on Multivariate Normal Distribution.\\r\\n\\r\\n    Args:\\r\\n        log_emlik: array like, shape (N, K).\\r\\n            contains the log likelihoods for each of N observations and\\r\\n            each of K distributions\\r\\n        weights:   weight vector for the K components in the mixture\\r\\n\\r\\n    Output:\\r\\n        gmmloglik: scalar, log likelihood of data given the GMM model.\\r\\n    \"\"\"\\r\\n    log_lik_gmm = 0\\r\\n    for obs in range(len(log_emlik)):\\r\\n        log_lik_gmm += logsumexp(log_emlik[obs, :] + np.log(weights))\\r\\n    return log_lik_gmm  \\r\\n    \\r\\n\\r\\ndef forward(log_emlik, log_startprob, log_transmat):\\r\\n    \"\"\"Forward (alpha) probabilities in log domain.\\r\\n\\r\\n    Args:\\r\\n        log_emlik: NxM array of emission log likelihoods, N frames, M states\\r\\n        log_startprob: log probability to start in state i\\r\\n        log_transmat: log transition probability from state i to j\\r\\n\\r\\n    Output:\\r\\n        forward_prob: NxM array of forward log probabilities for each of the M states in the model\\r\\n    \"\"\"\\r\\n    N, M = log_emlik.shape\\r\\n    forward_prob = np.zeros(log_emlik.shape)\\r\\n\\r\\n    forward_prob[0, :] = log_startprob[:-1] + log_emlik[0, :]\\r\\n\\r\\n    for n in range(1, N):\\r\\n        for j in range(M):\\r\\n            forward_prob[n, j] = logsumexp(forward_prob[n-1, :] + log_transmat[:-1, j]) + log_emlik[n, j]\\r\\n\\r\\n    return forward_prob\\r\\n\\r\\n\\r\\ndef backward(log_emlik, log_startprob, log_transmat):\\r\\n    \"\"\"Backward (beta) probabilities in log domain.\\r\\n\\r\\n    Args:\\r\\n        log_emlik: NxM array of emission log likelihoods, N frames, M states\\r\\n        log_startprob: log probability to start in state i\\r\\n        log_transmat: transition log probability from state i to j\\r\\n\\r\\n    Output:\\r\\n        backward_prob: NxM array of backward log probabilities for each of the M states in the model\\r\\n    \"\"\"\\r\\n    N, M = log_emlik.shape\\r\\n\\r\\n    backward_prob = np.zeros([N,M])\\r\\n\\r\\n    for i in reversed(range(N - 1)):\\r\\n        for j in range(M):\\r\\n            backward_prob[i,j] = logsumexp(log_transmat[j,:-1] + log_emlik[i+1,:] + backward_prob[i+1,:])\\r\\n\\r\\n    return backward_prob\\r\\n    \\r\\n\\r\\ndef viterbi(log_emlik, log_startprob, log_transmat, forceFinalState=True):\\r\\n    \"\"\"Viterbi path.\\r\\n\\r\\n    Args:\\r\\n        log_emlik: NxM array of emission log likelihoods, N frames, M states\\r\\n        log_startprob: log probability to start in state i\\r\\n        log_transmat: transition log probability from state i to j\\r\\n        forceFinalState: if True, start backtracking from the final state in\\r\\n                  the model, instead of the best state at the last time step\\r\\n\\r\\n    Output:\\r\\n        viterbi_loglik: log likelihood of the best path\\r\\n        viterbi_path: best path\\r\\n    \"\"\"\\r\\n    N, M = log_emlik.shape\\r\\n    \\r\\n    V = np.zeros([N, M])\\r\\n    B = np.zeros([N, M])\\r\\n\\r\\n    # Initialization\\r\\n    V[0, :] = log_startprob[:-1] + log_emlik[0, :] # Viterbi log likelihoods\\r\\n    B[0, :] = 0 # Viterbi indices\\r\\n\\r\\n    # Induction - propagating best paths forwards\\r\\n    for i in range(1, N): # i = observation\\r\\n        for j in range(M): # j = state \\r\\n            V[i][j] = np.max(V[i-1, :] + log_transmat[:-1, j]) + log_emlik[i][j]\\r\\n            B[i][j] = np.argmax(V[i-1, :] + log_transmat[:-1, j])\\r\\n\\r\\n    viterbi_loglik = np.max(V[-1, :])\\r\\n\\r\\n    # Backtracking \\r\\n    viterbi_path = np.zeros(N) # best path\\r\\n    viterbi_path[-1] = np.argmax(B[-1, :]) # start from last state\\r\\n    for i in reversed(range(N - 1)):\\r\\n        viterbi_path[i] = B[i+1, int(viterbi_path[i+1])] # add the rest\\r\\n\\r\\n    return viterbi_loglik, viterbi_path\\r\\n    \\r\\n    \\r\\n\\r\\ndef statePosteriors(log_alpha, log_beta):\\r\\n    \"\"\"State posterior (gamma) probabilities in log domain.\\r\\n\\r\\n    Args:\\r\\n        log_alpha: NxM array of log forward (alpha) probabilities\\r\\n        log_beta: NxM array of log backward (beta) probabilities\\r\\n    where N is the number of frames, and M the number of states\\r\\n\\r\\n    Output:\\r\\n        log_gamma: NxM array of gamma probabilities for each of the M states in the model\\r\\n    \"\"\"\\r\\n    return log_alpha + log_beta - logsumexp(log_alpha[-1,:])\\r\\n\\r\\n\\r\\ndef updateMeanAndVar(X, log_gamma, varianceFloor=5.0):\\r\\n    \"\"\" Update Gaussian parameters with diagonal covariance\\r\\n\\r\\n    Args:\\r\\n         X: NxD array of feature vectors\\r\\n         log_gamma: NxM state posterior probabilities in log domain\\r\\n         varianceFloor: minimum allowed variance scalar\\r\\n    were N is the lenght of the observation sequence, D is the\\r\\n    dimensionality of the feature vectors and M is the number of\\r\\n    states in the model\\r\\n\\r\\n    Outputs:\\r\\n         means: MxD mean vectors for each state\\r\\n         covars: MxD covariance (variance) vectors for each state\\r\\n    \"\"\"\\r\\n    N, M = log_gamma.shape\\r\\n    D = X.shape[1]\\r\\n    \\r\\n    means  = np.zeros([M, D])\\r\\n    covars = np.zeros([M, D])\\r\\n    gamma = np.exp(log_gamma)\\r\\n    \\r\\n    for i in range(M):\\r\\n        means[i, :] = np.dot(X.T, gamma[:, i]) / np.sum(gamma[:, i])\\r\\n        x = X.T - means[i,:].reshape([D, 1])\\r\\n\\r\\n        result = 0\\r\\n        for j in range(N):\\r\\n            result = result + gamma[j, i] * np.outer(x[:, j], x[:, j])\\r\\n\\r\\n        covars[i, :] = np.diag(result) / np.sum(gamma[:, i])\\r\\n\\r\\n    covars[covars < varianceFloor] = varianceFloor\\r\\n\\r\\n    return means, covars\\r\\n\\r\\n\\r\\ndef compare(frames, example_frames):\\r\\n    \"\"\" Returns True if frames and example_frame are equal \"\"\"\\r\\n    \\r\\n    return np.isclose(frames, example_frames).all()',\n",
              " 'lab2_tools.py': b'import numpy as np\\r\\n\\r\\ndef logsumexp(arr, axis=0):\\r\\n    \"\"\"Computes the sum of arr assuming arr is in the log domain.\\r\\n    Returns log(sum(exp(arr))) while minimizing the possibility of\\r\\n    over/underflow.\\r\\n    \"\"\"\\r\\n    arr = np.rollaxis(arr, axis)\\r\\n    # Use the max to normalize, as with the log this is what accumulates\\r\\n    # the less errors\\r\\n    vmax = arr.max(axis=0)\\r\\n    if vmax.ndim > 0:\\r\\n        vmax[~np.isfinite(vmax)] = 0\\r\\n    elif not np.isfinite(vmax):\\r\\n        vmax = 0\\r\\n    with np.errstate(divide=\"ignore\"):\\r\\n        out = np.log(np.sum(np.exp(arr - vmax), axis=0))\\r\\n        out += vmax\\r\\n        return out\\r\\n\\r\\ndef log_multivariate_normal_density_diag(X, means, covars):\\r\\n    \"\"\"Compute Gaussian log-density at X for a diagonal model\\r\\n\\r\\n    Args:\\r\\n        X: array like, shape (n_observations, n_features)\\r\\n        means: array like, shape (n_components, n_features)\\r\\n        covars: array like, shape (n_components, n_features)\\r\\n\\r\\n    Output:\\r\\n        lpr: array like, shape (n_observations, n_components)\\r\\n    From scikit-learn/sklearn/mixture/gmm.py\\r\\n    \"\"\"\\r\\n    n_samples, n_dim = X.shape\\r\\n    lpr = -0.5 * (n_dim * np.log(2 * np.pi) + np.sum(np.log(covars), 1)\\r\\n                  + np.sum((means ** 2) / covars, 1)\\r\\n                  - 2 * np.dot(X, (means / covars).T)\\r\\n                  + np.dot(X ** 2, (1.0 / covars).T))\\r\\n    return lpr\\r\\n',\n",
              " 'lab3_proto.py': b'import numpy as np\\r\\nfrom lab3_tools import *\\r\\n\\r\\ndef words2phones(wordList, pronDict, addSilence=True, addShortPause=True):\\r\\n   \"\"\" word2phones: converts word level to phone level transcription adding silence\\r\\n\\r\\n   Args:\\r\\n      wordList: list of word symbols\\r\\n      pronDict: pronunciation dictionary. The keys correspond to words in wordList\\r\\n      addSilence: if True, add initial and final silence\\r\\n      addShortPause: if True, add short pause model \"sp\" at end of each word\\r\\n   Output:\\r\\n      list of phone symbols\\r\\n   \"\"\"\\r\\n\\r\\n   phoneSymb = []\\r\\n\\r\\n   for digit in wordList:\\r\\n      phoneSymb += pronDict[digit]\\r\\n      if addShortPause: phoneSymb += [\\'sp\\'] \\r\\n\\r\\n   if addSilence: phoneSymb = [\\'sil\\'] + phoneSymb + [\\'sil\\']\\r\\n\\r\\n   return phoneSymb\\r\\n\\r\\n\\r\\ndef forcedAlignment(lmfcc, phoneHMMs, phoneTrans):\\r\\n    \"\"\" forcedAlignmen: aligns a phonetic transcription at the state level\\r\\n\\r\\n    Args:\\r\\n       lmfcc: NxD array of MFCC feature vectors (N vectors of dimension D)\\r\\n              computed the same way as for the training of phoneHMMs\\r\\n       phoneHMMs: set of phonetic Gaussian HMM models\\r\\n       phoneTrans: list of phonetic symbols to be aligned including initial and\\r\\n                   final silence\\r\\n\\r\\n    Returns:\\r\\n       list of strings in the form phoneme_index specifying, for each time step\\r\\n       the state from phoneHMMs corresponding to the viterbi path.\\r\\n    \"\"\"\\r\\n\\r\\ndef hmmLoop(hmmmodels, namelist=None):\\r\\n    \"\"\" Combines HMM models in a loop\\r\\n\\r\\n    Args:\\r\\n       hmmmodels: list of dictionaries with the following keys:\\r\\n           name: phonetic or word symbol corresponding to the model\\r\\n           startprob: M+1 array with priori probability of state\\r\\n           transmat: (M+1)x(M+1) transition matrix\\r\\n           means: MxD array of mean vectors\\r\\n           covars: MxD array of variances\\r\\n       namelist: list of model names that we want to combine, if None,\\r\\n                 all the models in hmmmodels are used\\r\\n\\r\\n    D is the dimension of the feature vectors\\r\\n    M is the number of emitting states in each HMM model (could be\\r\\n      different in each model)\\r\\n\\r\\n    Output\\r\\n       combinedhmm: dictionary with the same keys as the input but\\r\\n                    combined models\\r\\n       stateMap: map between states in combinedhmm and states in the\\r\\n                 input models.\\r\\n\\r\\n    Examples:\\r\\n       phoneLoop = hmmLoop(phoneHMMs)\\r\\n       wordLoop = hmmLoop(wordHMMs, [\\'o\\', \\'z\\', \\'1\\', \\'2\\', \\'3\\'])\\r\\n    \"\"\"\\r\\n',\n",
              " 'lab3_tools.py': b'import numpy as np\\r\\nimport os\\r\\nfrom pysndfile import sndio\\r\\n\\r\\ndef path2info(path):\\r\\n    \"\"\"\\r\\n    path2info: parses paths in the TIDIGIT format and extracts information\\r\\n               about the speaker and the utterance\\r\\n\\r\\n    Example:\\r\\n    path2info(\\'tidigits/disc_4.1.1/tidigits/train/man/ae/z9z6531a.wav\\')\\r\\n    \"\"\"\\r\\n    rest, filename = os.path.split(path)\\r\\n    rest, speakerID = os.path.split(rest)\\r\\n    rest, gender = os.path.split(rest)\\r\\n    digits = filename[:-5]\\r\\n    repetition = filename[-5]\\r\\n    return gender, speakerID, digits, repetition\\r\\n\\r\\ndef loadAudio(filename):\\r\\n    \"\"\"\\r\\n    loadAudio: loads audio data from file using pysndfile\\r\\n\\r\\n    Note that, by default pysndfile converts the samples into floating point\\r\\n    numbers and rescales them in the range [-1, 1]. This is avoided by specifying\\r\\n    the option dtype=np.int16 which keeps both the original data type and range\\r\\n    of values.\\r\\n    \"\"\"\\r\\n    sndobj = sndio.read(filename, dtype=np.int16)\\r\\n    samplingrate = sndobj[1]\\r\\n    samples = np.array(sndobj[0])\\r\\n    return samples, samplingrate\\r\\n\\r\\ndef frames2trans(sequence, outfilename=None, timestep=0.01):\\r\\n    \"\"\"\\r\\n    Outputs a standard transcription given a frame-by-frame\\r\\n    list of strings.\\r\\n\\r\\n    Example (using functions from Lab 1 and Lab 2):\\r\\n    phones = [\\'sil\\', \\'sil\\', \\'sil\\', \\'ow\\', \\'ow\\', \\'ow\\', \\'ow\\', \\'ow\\', \\'sil\\', \\'sil\\']\\r\\n    trans = frames2trans(phones, \\'oa.lab\\')\\r\\n\\r\\n    Then you can use, for example wavesurfer to open the wav file and the transcription\\r\\n    \"\"\"\\r\\n    sym = sequence[0]\\r\\n    start = 0\\r\\n    end = 0\\r\\n    trans = \\'\\'\\r\\n    for t in range(len(sequence)):\\r\\n        if sequence[t] != sym:\\r\\n            trans = trans + str(start) + \\' \\' + str(end) + \\' \\' + sym + \\'\\\\n\\'\\r\\n            sym = sequence[t]\\r\\n            start = end\\r\\n        end = end + timestep\\r\\n    trans = trans + str(start) + \\' \\' + str(end) + \\' \\' + sym + \\'\\\\n\\'\\r\\n    if outfilename != None:\\r\\n        with open(outfilename, \\'w\\') as f:\\r\\n            f.write(trans)\\r\\n    return trans\\r\\n\\r\\n        \\r\\n',\n",
              " 'prondict.py': b\"prondict = {} \\r\\nprondict['o'] = ['ow']\\r\\nprondict['z'] = ['z', 'iy', 'r', 'ow']\\r\\nprondict['1'] = ['w', 'ah', 'n']\\r\\nprondict['2'] = ['t', 'uw']\\r\\nprondict['3'] = ['th', 'r', 'iy']\\r\\nprondict['4'] = ['f', 'ao', 'r']\\r\\nprondict['5'] = ['f', 'ay', 'v']\\r\\nprondict['6'] = ['s', 'ih', 'k', 's']\\r\\nprondict['7'] = ['s', 'eh', 'v', 'ah', 'n']\\r\\nprondict['8'] = ['ey', 't']\\r\\nprondict['9'] = ['n', 'ay', 'n']\\r\\n\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OMZ3hM9z-L9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from lab1_proto import *\n",
        "from lab2_proto import *\n",
        "from lab3_proto import *\n",
        "from lab1_tools import *\n",
        "from lab2_tools import *\n",
        "from lab3_tools import *\n",
        "from prondict import prondict\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnP2-oIL_IEX",
        "colab_type": "text"
      },
      "source": [
        "## Check examples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HnRiMcwA7d5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example = np.load('/content/drive/My Drive/Lab3/lab3_example.npz', allow_pickle=True)['example'].item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dknvV5JBRyl4",
        "colab_type": "text"
      },
      "source": [
        "# 4.1 Target Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6ScIE_E0tFj",
        "colab_type": "code",
        "outputId": "5a3c33b4-d176-4680-f378-4e05c2750a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "phoneHMMs = np.load('/content/drive/My Drive/Lab3/lab2_models_all.npz', allow_pickle=True)['phoneHMMs'].item()\n",
        "phones = sorted(phoneHMMs.keys())\n",
        "nstates = {phone: phoneHMMs[phone]['means'].shape[0] for phone in phones}\n",
        "stateList = [ph + '_' + str(id) for ph in phones for id in range(nstates[ph])]\n",
        "print(stateList)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ah_0', 'ah_1', 'ah_2', 'ao_0', 'ao_1', 'ao_2', 'ay_0', 'ay_1', 'ay_2', 'eh_0', 'eh_1', 'eh_2', 'ey_0', 'ey_1', 'ey_2', 'f_0', 'f_1', 'f_2', 'ih_0', 'ih_1', 'ih_2', 'iy_0', 'iy_1', 'iy_2', 'k_0', 'k_1', 'k_2', 'n_0', 'n_1', 'n_2', 'ow_0', 'ow_1', 'ow_2', 'r_0', 'r_1', 'r_2', 's_0', 's_1', 's_2', 'sil_0', 'sil_1', 'sil_2', 'sp_0', 't_0', 't_1', 't_2', 'th_0', 'th_1', 'th_2', 'uw_0', 'uw_1', 'uw_2', 'v_0', 'v_1', 'v_2', 'w_0', 'w_1', 'w_2', 'z_0', 'z_1', 'z_2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SCaR5JzR31i",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 Forced Alignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSLjfSKe4DKw",
        "colab_type": "code",
        "outputId": "8bf9ba0b-1b5d-4241-c4f9-e01dcf5c0fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "filename = '/content/drive/My Drive/tidigits/disc_4.1.1/tidigits/train/man/nw/z43a.wav'\n",
        "samples, samplingrate = loadAudio(filename)\n",
        "lmfcc = mfcc(samples)\n",
        "print(\"LMFCC Shape: \", lmfcc.shape)\n",
        "\n",
        "wordTrans = list(path2info(filename)[2])\n",
        "print(\"Sequence of digits: \", wordTrans)\n",
        "\n",
        "print(\"Pronunciation dictionary: \", prondict)\n",
        "\n",
        "phoneTrans = words2phones(wordTrans, prondict)\n",
        "print(\"Phone level transcription: \", phoneTrans)\n",
        "utteranceHMM = concatHMMs(phoneHMMs, phoneTrans)\n",
        "\n",
        "stateTrans = [phone + '_' + str(stateid) for phone in phoneTrans\n",
        "              for stateid in range(nstates[phone])]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LMFCC Shape:  (178, 13)\n",
            "Sequence of digits:  ['z', '4', '3']\n",
            "Pronunciation dictionary:  {'o': ['ow'], 'z': ['z', 'iy', 'r', 'ow'], '1': ['w', 'ah', 'n'], '2': ['t', 'uw'], '3': ['th', 'r', 'iy'], '4': ['f', 'ao', 'r'], '5': ['f', 'ay', 'v'], '6': ['s', 'ih', 'k', 's'], '7': ['s', 'eh', 'v', 'ah', 'n'], '8': ['ey', 't'], '9': ['n', 'ay', 'n']}\n",
            "Phone level transcription:  ['sil', 'z', 'iy', 'r', 'ow', 'sp', 'f', 'ao', 'r', 'sp', 'th', 'r', 'iy', 'sp', 'sil']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFYkWvFo_Qjl",
        "colab_type": "code",
        "outputId": "a5f8d99c-1db9-4ce8-ec21-641d1b913721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Compare with examples\n",
        "for i in example['utteranceHMM'].keys():\n",
        "    print(compare(utteranceHMM[i], example['utteranceHMM'][i]))\n",
        "\n",
        "obsloglik = log_multivariate_normal_density_diag(lmfcc, utteranceHMM['means'], utteranceHMM['covars'])\n",
        "viterbi_loglik, viterbi_path = viterbi(obsloglik, np.log(utteranceHMM['startprob']), np.log(utteranceHMM['transmat']))\n",
        "viterbiStateTrans = [stateTrans[idx] for idx in viterbi_path.astype(np.int64)]\n",
        "\n",
        "print('Comparing obsloglik...', compare(obsloglik, example['obsloglik']))\n",
        "print('Comparing viterbiPath...', compare(viterbi_path, example['viterbiPath']))\n",
        "print('Comparing viterbiStateTrans...', np.all(viterbiStateTrans == example['viterbiStateTrans']))\n",
        "\n",
        "frames = frames2trans(viterbiStateTrans, outfilename= ''.join(path2info(filename)[2:]) + '.lab')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "Comparing obsloglik... True\n",
            "Comparing viterbiPath... True\n",
            "Comparing viterbiStateTrans... True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7WOETbIRsXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from IPython.display import Image\n",
        "#Image(filename='/content/drive/My Drive/z43a_wavesurfer.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "addeh9oShjCY",
        "colab_type": "text"
      },
      "source": [
        "# 4.3 Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD6858tUPETU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features_and_targets(filename):\n",
        "    \"\"\" extract_features_and_targets: extracts lmfcc, mspecc and \n",
        "        targets from a *.wav sound file\n",
        "\n",
        "    Args:\n",
        "        filename: location of .wav file\n",
        "\n",
        "    Returns:\n",
        "        lmfcc: liftered mfcc features\n",
        "        mspec: filterbank features\n",
        "        targets: indices of the target state for each feature\n",
        "    \"\"\"\n",
        "    samples, samplingrate = loadAudio(filename)\n",
        "    wordTrans = list(path2info(filename)[2])\n",
        "    phoneTrans = words2phones(wordTrans, prondict)\n",
        "    stateTrans = [phone + '_' + str(stateid) for phone in phoneTrans \n",
        "                  for stateid in range(nstates[phone])]\n",
        "    utteranceHMM = concatHMMs(phoneHMMs, phoneTrans)\n",
        "\n",
        "    stateTrans = [phone + '_' + str(stateid) for phone in phoneTrans\n",
        "              for stateid in range(nstates[phone])]\n",
        "\n",
        "    lmfcc = mfcc(samples)\n",
        "    mspec_ = mspec(samples)\n",
        "\n",
        "    obsloglik = log_multivariate_normal_density_diag(lmfcc, utteranceHMM['means'], utteranceHMM['covars'])\n",
        "    viterbi_loglik, viterbi_path = viterbi(obsloglik, np.log(utteranceHMM['startprob']), np.log(utteranceHMM['transmat']), True)\n",
        "    targets = [stateTrans[idx] for idx in viterbi_path.astype(np.int64)] \n",
        "\n",
        "    return lmfcc, mspec_, targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I1uUxkc-Tn1",
        "colab_type": "text"
      },
      "source": [
        "Extract and save lmfcc and mspec features, and their associated targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUkLiuEhhf04",
        "colab_type": "code",
        "outputId": "4055de68-6d77-4416-972b-d3617015ca71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"\n",
        "from tqdm import tqdm\n",
        "traindata = []\n",
        "for root, dirs, files in tqdm(os.walk('/content/drive/My Drive/tidigits/disc_4.1.1/tidigits/train'), desc='dirs'):\n",
        "  for file in files:\n",
        "    if file.endswith('.wav'):\n",
        "      filename = os.path.join(root, file)\n",
        "      samples, samplingrate = loadAudio(filename)\n",
        "      lmfcc, mspec_, targets =  extract_features_and_targets(filename)\n",
        "      traindata.append({'filename': filename, 'lmfcc': lmfcc,\n",
        "                        'mspec': mspec_, 'targets': targets})\n",
        "\n",
        "np.savez('/content/drive/My Drive/Lab3/traindata.npz', traindata=traindata)\n",
        "\"\"\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfrom tqdm import tqdm\\ntraindata = []\\nfor root, dirs, files in tqdm(os.walk('/content/drive/My Drive/tidigits/disc_4.1.1/tidigits/train'), desc='dirs'):\\n  for file in files:\\n    if file.endswith('.wav'):\\n      filename = os.path.join(root, file)\\n      samples, samplingrate = loadAudio(filename)\\n      lmfcc, mspec_, targets =  extract_features_and_targets(filename)\\n      traindata.append({'filename': filename, 'lmfcc': lmfcc,\\n                        'mspec': mspec_, 'targets': targets})\\n\\nnp.savez('/content/drive/My Drive/Lab3/traindata.npz', traindata=traindata)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9pxdzXZbVU2",
        "colab_type": "code",
        "outputId": "63dc2251-9cda-442e-dc34-8b382f01537f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"\n",
        "testdata = []\n",
        "for root, dirs, files in tqdm(os.walk('/content/drive/My Drive/tidigits/disc_4.2.1/tidigits/test'), desc='dirs'):\n",
        "  for file in files:\n",
        "    if file.endswith('.wav'):\n",
        "      filename = os.path.join(root, file)\n",
        "      samples, samplingrate = loadAudio(filename)\n",
        "      lmfcc, mspec_, targets =  extract_features_and_targets(filename)\n",
        "      testdata.append({'filename': filename, 'lmfcc': lmfcc,\n",
        "                        'mspec': mspec_, 'targets': targets})\n",
        "\n",
        "np.savez('/content/drive/My Drive/Lab3/testdata.npz', testdata=testdata)\n",
        "\"\"\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntestdata = []\\nfor root, dirs, files in tqdm(os.walk('/content/drive/My Drive/tidigits/disc_4.2.1/tidigits/test'), desc='dirs'):\\n  for file in files:\\n    if file.endswith('.wav'):\\n      filename = os.path.join(root, file)\\n      samples, samplingrate = loadAudio(filename)\\n      lmfcc, mspec_, targets =  extract_features_and_targets(filename)\\n      testdata.append({'filename': filename, 'lmfcc': lmfcc,\\n                        'mspec': mspec_, 'targets': targets})\\n\\nnp.savez('/content/drive/My Drive/Lab3/testdata.npz', testdata=testdata)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE1sI669iKqD",
        "colab_type": "text"
      },
      "source": [
        "# 4.4 Training and Validation Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJh3VAsecThl",
        "colab_type": "text"
      },
      "source": [
        "##### Split the training data into a training set (roughly 90%) and validation set (remaining 10%). Make sure that there is a similar distribution of men and women in both sets, and that each speaker is only included in one of the two sets. The last requirement is to ensure that we do not get artificially good results on the validation set. Explain how you selected the two data sets.\n",
        "\n",
        "*** You should specify both the training and validation data with the respective targets. What is the purpose of the validation data? **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gtbsZ2lce_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = np.load('/content/drive/My Drive/Lab3/traindata.npz', allow_pickle=True)['traindata']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19A9940o3jCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_val_split(data):\n",
        "  \"\"\"train_val_split: splits dataset into train and validation sets\n",
        "\n",
        "  Args:\n",
        "    data: dataset to be split\n",
        "  \"\"\"\n",
        "  MAN_SAMPLES = 4235\n",
        "  WOMAN_SAMPLES = 4388\n",
        "  VAL_PER = 0.1\n",
        "  SAMPLES_PER_PERSON = 77\n",
        "\n",
        "  nsamples = len(data)\n",
        "  approx_val_size = int(0.1 * nsamples)\n",
        "  val_class_people_count = int(approx_val_size / (SAMPLES_PER_PERSON))\n",
        "\n",
        "  if val_class_people_count % 2 != 0:\n",
        "      val_class_people_count += 1\n",
        "\n",
        "  val_size = val_class_people_count * SAMPLES_PER_PERSON\n",
        "  training_size = nsamples - val_size\n",
        "\n",
        "  samples_per_gender = int(val_size / 2)\n",
        "\n",
        "  val_data = [data[i] for i in range(0, samples_per_gender)]\n",
        "  val_data.extend([data[i] for i in range(MAN_SAMPLES, MAN_SAMPLES + samples_per_gender)])\n",
        "  training_data = [sample for sample in data if sample['filename'] not in [x['filename'] for x in val_data]]\n",
        "\n",
        "  np.save('/content/drive/My Drive/Lab3/training_data.npy', training_data)\n",
        "  np.save('/content/drive/My Drive/Lab3/validation_data.npy', val_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvI0fx055Yky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_val_split(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoY6WbNkiV84",
        "colab_type": "text"
      },
      "source": [
        "# 4.5 Acoustic Context (Dynamic Features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LirZzxUH6Ccv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = np.load('/content/drive/My Drive/Lab3/training_data.npy', allow_pickle=True)\n",
        "val_data = np.load('/content/drive/My Drive/Lab3/validation_data.npy', allow_pickle=True)\n",
        "test_data = np.load('/content/drive/My Drive/Lab3/testdata.npz', allow_pickle=True)['testdata']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw1hdtUBiU1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each utterance and time step, stack 7 MFCC or filterbank features symmetrically distributed\n",
        "# around the current time step. That is, at time n, stack the features at times [n-3; n-\n",
        "# 2; n-1; n; n+1; n+2; n+3]). At the beginning and end of each utterance, use mirrored feature\n",
        "# vectors in place of the missing vectors. For example at the beginning use feature vectors with\n",
        "# indexes [3; 2; 1; 0; 1; 2; 3] for the first time step, [2; 1; 0; 1; 2; 3; 4] for the second time step, and so\n",
        "# on. The “boundary effect” is usually not very important because each utterance begins and ends\n",
        "# with silence."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC93bO4N_0nB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "def create_dynamic_features(data):\n",
        "    \"\"\" create_dynamic_features: Creates dynamic features by concatinated 7 \n",
        "        lmfcc or mspec featues together for each step\n",
        "\n",
        "    Args:\n",
        "        data: list of dictionaries with keys: 'lmfcc', 'mspec' and 'targets'\n",
        "    \n",
        "    Returns:\n",
        "        lmfcc_features: [NxD_lmfcc * 7] where N = number of all concatinated samples of all words in data\n",
        "        mspec_features: [NxD_mspec * 7] where N = of all concatinated samples of all words in data\n",
        "        targets: [N,] index of state for each sample\n",
        "    \"\"\"\n",
        "    \n",
        "    D_lmfcc = data[0]['lmfcc'].shape[1]\n",
        "    D_mspec = data[0]['mspec'].shape[1]\n",
        "    N = sum([len(x['targets']) for x in data])\n",
        "\n",
        "    # Features to be returned\n",
        "    lmfcc_features = np.zeros((N, D_lmfcc * 7))\n",
        "    mspec_features = np.zeros((N, D_mspec * 7))\n",
        "    \n",
        "    # Targets to be returned\n",
        "    targets = []\n",
        "\n",
        "    # through all data\n",
        "    k = 0\n",
        "    for x in tqdm(data): \n",
        "        times, dim = x['lmfcc'].shape\n",
        "        # for each time step\n",
        "        for i in range(times):\n",
        "            if i < 3 or i >= times - 3:\n",
        "                lmfcc_features[k, :] = np.hstack(np.pad(x['lmfcc'], pad_width=((3, 3), (0, 0)), mode='reflect')[i:i+7, :])\n",
        "                mspec_features[k, :] = np.hstack(np.pad(x['mspec'], pad_width=((3, 3), (0, 0)), mode='reflect')[i:i+7, :])\n",
        "            else:\n",
        "                lmfcc_features[k,:] = np.hstack(x['lmfcc'][i-3:i+4, :])\n",
        "                mspec_features[k,:] = np.hstack(x['mspec'][i-3:i+4, :])\n",
        "            k +=1\n",
        "        targets = targets + x['targets']\n",
        "\n",
        "    return lmfcc_features, mspec_features, targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6rZ9PAvCAak",
        "colab_type": "code",
        "outputId": "4b27855e-dd3b-4891-90da-b1633a8761b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "lmfcc_train_x, mspec_train_x, train_y = create_dynamic_features(train_data)\n",
        "lmfcc_val_x, mspec_val_x, val_y = create_dynamic_features(val_data)\n",
        "lmfcc_test_x, mspec_test_x, test_y = create_dynamic_features(test_data)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7699/7699 [01:01<00:00, 125.68it/s]\n",
            "100%|██████████| 924/924 [00:04<00:00, 196.25it/s]\n",
            "100%|██████████| 8700/8700 [01:18<00:00, 111.40it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9vUqa_2jmsP",
        "colab_type": "text"
      },
      "source": [
        "# 4.6 Feature Standardisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gob6Vu61tmT",
        "colab_type": "text"
      },
      "source": [
        "Normalization over the complete training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p718kAvkjlul",
        "colab_type": "code",
        "outputId": "9bb48d41-f455-4156-f36a-a282f92f1ba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\"\"\"\n",
        "print('Train data before feature standardization: ', lmfcc_train_x[0])\n",
        "print('Validation data before feature standardization: ', lmfcc_val_x[0])\n",
        "print('Test data before feature standardization: ', lmfcc_test_x[0])\n",
        "\"\"\"\n",
        "scaler = StandardScaler()\n",
        "scaler = scaler.fit(lmfcc_train_x)\n",
        "lmfcc_train_x = scaler.transform(lmfcc_train_x)\n",
        "lmfcc_val_x = scaler.transform(lmfcc_val_x)\n",
        "lmfcc_test_x = scaler.transform(lmfcc_val_x)\n",
        "\"\"\"\n",
        "print('Train data after feature standardization: ', lmfcc_train_x[0])\n",
        "print('Validation data after feature standardization: ', lmfcc_val_x[0])\n",
        "print('Test data after feature standardization: ', lmfcc_test_x[0])\n",
        "\"\"\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data before feature standardization:  [ 4.45740695e+01 -9.70452188e+01  9.89285496e+01  1.30110535e+01\n",
            " -2.22716868e+01 -8.93713732e+01 -1.93372222e+01 -2.46926385e+02\n",
            " -2.42524656e-01  1.21487772e+02 -1.17587658e+01  1.40877864e+01\n",
            " -1.64728422e+01  2.03107423e+01 -1.58162304e+02  1.29291883e+02\n",
            "  6.78660270e+01  8.23696302e+01  3.94972794e+01 -9.39323913e+01\n",
            " -2.08224406e+02 -7.76468906e+01  2.08048811e+02 -9.59758683e+01\n",
            " -1.62394674e+02  9.61689525e+01  3.30028013e+01 -1.44811196e+02\n",
            "  1.31217981e+02  1.65058619e+01  6.52676127e+01  3.08077257e+01\n",
            " -1.77337209e+02 -1.81096519e+02 -5.86573894e+01 -3.31812615e+01\n",
            " -1.27537435e+01  3.48484909e+01  1.08079158e+02  2.25167437e+01\n",
            " -1.41375676e+02  1.01894601e+02  7.60806414e+01 -3.51136995e+01\n",
            " -1.03045435e+02 -6.94672047e+00 -1.14077622e+02 -1.43544505e+02\n",
            " -5.66687693e+01 -7.28719949e+00 -3.18410929e+01  6.32642360e+01\n",
            "  3.30028013e+01 -1.44811196e+02  1.31217981e+02  1.65058619e+01\n",
            "  6.52676127e+01  3.08077257e+01 -1.77337209e+02 -1.81096519e+02\n",
            " -5.86573894e+01 -3.31812615e+01 -1.27537435e+01  3.48484909e+01\n",
            "  1.08079158e+02  2.03107423e+01 -1.58162304e+02  1.29291883e+02\n",
            "  6.78660270e+01  8.23696302e+01  3.94972794e+01 -9.39323913e+01\n",
            " -2.08224406e+02 -7.76468906e+01  2.08048811e+02 -9.59758683e+01\n",
            " -1.62394674e+02  9.61689525e+01  4.45740695e+01 -9.70452188e+01\n",
            "  9.89285496e+01  1.30110535e+01 -2.22716868e+01 -8.93713732e+01\n",
            " -1.93372222e+01 -2.46926385e+02 -2.42524656e-01  1.21487772e+02\n",
            " -1.17587658e+01  1.40877864e+01 -1.64728422e+01]\n",
            "Validation data before feature standardization:  [ 8.73940337e+01 -1.36538869e+02 -6.24781009e+01  1.52737673e+02\n",
            "  5.42908613e+01 -1.16551752e+02  6.51878455e+01  3.02947381e+01\n",
            "  1.56387134e+02  6.17135116e+01  2.81732216e+00  1.04723211e+01\n",
            "  4.89719909e+01 -2.95693958e+01 -1.90239233e+02  4.59200751e+01\n",
            "  3.07134293e+01  3.63241945e+01 -8.98798870e+01 -1.80344105e+01\n",
            "  4.28457324e+01  9.92677805e+00  1.54472850e+02  7.38573304e+01\n",
            " -8.17472646e+01  3.68646192e+01 -3.88969709e+01 -2.13616964e+02\n",
            "  6.28871581e+01 -1.00359186e-01  3.75366083e+01  1.23767322e+02\n",
            "  5.62679738e+01  1.79745927e+01  1.20938608e+02  6.28265979e+01\n",
            "  4.62529221e+01  1.59364832e+01  1.20050613e+02  1.27806211e+01\n",
            " -2.18935593e+02 -9.88812243e+01  3.49833926e+01  1.29411354e+02\n",
            " -2.96830639e+01 -4.53647536e+01  1.33712261e+02  6.00630953e+01\n",
            " -7.70870492e+01 -1.38218418e+02  1.38200095e+02  8.07197529e+01\n",
            " -3.88969709e+01 -2.13616964e+02  6.28871581e+01 -1.00359186e-01\n",
            "  3.75366083e+01  1.23767322e+02  5.62679738e+01  1.79745927e+01\n",
            "  1.20938608e+02  6.28265979e+01  4.62529221e+01  1.59364832e+01\n",
            "  1.20050613e+02 -2.95693958e+01 -1.90239233e+02  4.59200751e+01\n",
            "  3.07134293e+01  3.63241945e+01 -8.98798870e+01 -1.80344105e+01\n",
            "  4.28457324e+01  9.92677805e+00  1.54472850e+02  7.38573304e+01\n",
            " -8.17472646e+01  3.68646192e+01  8.73940337e+01 -1.36538869e+02\n",
            " -6.24781009e+01  1.52737673e+02  5.42908613e+01 -1.16551752e+02\n",
            "  6.51878455e+01  3.02947381e+01  1.56387134e+02  6.17135116e+01\n",
            "  2.81732216e+00  1.04723211e+01  4.89719909e+01]\n",
            "Test data before feature standardization:  [  -6.95901671 -166.2530995    43.47795964   24.63269872   -4.39692265\n",
            "  -63.71863191 -110.92126223 -146.82593737  -43.45230034  -88.07034397\n",
            " -142.30546878  -24.45995333   39.80666357   24.88871374 -145.43991987\n",
            "   26.65264868   -2.2941692   -67.22283908  -13.38296373  -16.96133351\n",
            "   -3.76804017  -65.40087047 -228.39656863 -165.05429703    9.31772\n",
            "   87.32137463   10.40191189 -180.22297287   25.8122306    27.98629489\n",
            "   78.03399796   44.63941164  -91.73315498  -87.08036207  -40.06711295\n",
            " -110.55022237 -109.17319058   56.24537706   27.86237852   -8.59347472\n",
            " -173.34139312   31.40575119   33.87793998   67.0393664    28.45602744\n",
            "  -28.68560864  -77.48016249  -71.15945364  -96.79839484 -107.38699442\n",
            "   73.03966041   91.59460983   10.40191189 -180.22297287   25.8122306\n",
            "   27.98629489   78.03399796   44.63941164  -91.73315498  -87.08036207\n",
            "  -40.06711295 -110.55022237 -109.17319058   56.24537706   27.86237852\n",
            "   24.88871374 -145.43991987   26.65264868   -2.2941692   -67.22283908\n",
            "  -13.38296373  -16.96133351   -3.76804017  -65.40087047 -228.39656863\n",
            " -165.05429703    9.31772      87.32137463   -6.95901671 -166.2530995\n",
            "   43.47795964   24.63269872   -4.39692265  -63.71863191 -110.92126223\n",
            " -146.82593737  -43.45230034  -88.07034397 -142.30546878  -24.45995333\n",
            "   39.80666357]\n",
            "Train data after feature standardization:  [-1.37108364 -0.60452847  0.75190502 -0.5300535  -0.05707614 -0.70164744\n",
            "  0.05589726 -1.44162203  0.01201227  0.89136391 -0.19952421  0.32761484\n",
            " -0.41097297 -1.44515143 -0.89177127  0.94293827 -0.27247515  0.51028413\n",
            "  0.11736617 -0.37452005 -1.21704941 -0.44793873  1.3958202  -0.75051695\n",
            " -0.86838321  0.43321361 -1.40630234 -0.83016777  0.9529711  -0.51492659\n",
            "  0.41631538  0.06076114 -0.85546251 -1.06001269 -0.33638025 -0.01608418\n",
            " -0.20848513  0.46765783  0.52216789 -1.43793939 -0.81488491  0.76442759\n",
            " -0.23491798 -0.12957946 -0.79322329  0.12219878 -0.66995587 -0.84072988\n",
            " -0.15497667 -0.17395414  0.01546028  0.18552327 -1.40566378 -0.8318497\n",
            "  0.94849521 -0.51613626  0.41415435  0.05804121 -0.85855955 -1.06251495\n",
            " -0.33830829 -0.01921079 -0.21080971  0.46688215  0.52120471 -1.44424727\n",
            " -0.89543735  0.9341199  -0.27469546  0.50584252  0.11213216 -0.3809425\n",
            " -1.22222184 -0.45200465  1.38926211 -0.75551875 -0.86942964  0.43134255\n",
            " -1.3703584  -0.60977929  0.73937572 -0.5338241  -0.06272255 -0.71067654\n",
            "  0.0458922  -1.44956242  0.00591703  0.88148682 -0.20682628  0.32573233\n",
            " -0.41430539]\n",
            "Validation data after feature standardization:  [-1.2405116  -0.78944919 -0.27477825  0.12759362  0.35888872 -0.87466862\n",
            "  0.54189693  0.17688762  0.94057079  0.54187663 -0.10439115  0.30311836\n",
            "  0.07974313 -1.59726375 -1.04202354  0.41284036 -0.44738039  0.26020543\n",
            " -0.70639664  0.0617821   0.24869573  0.07125358  1.08259414  0.35807335\n",
            " -0.3219986  -0.01150174 -1.62555992 -1.15257438  0.51867602 -0.59312344\n",
            "  0.26575375  0.65279042  0.487185    0.10213723  0.72847524  0.54520268\n",
            "  0.17672801  0.33953302  0.61194955 -1.46762646 -1.17842386 -0.51119963\n",
            " -0.42848483  0.76341012 -0.3259078  -0.09857834  0.77655544  0.36662278\n",
            " -0.2743443  -1.02884334  1.1673452   0.31645781 -1.62487887 -1.1544572\n",
            "  0.51450531 -0.59436906  0.26368692  0.65036018  0.48367251  0.0995735\n",
            "  0.72671428  0.54203692  0.17450666  0.33877542  0.61101886 -1.59632578\n",
            " -1.0458892   0.40478359 -0.44975855  0.25608366 -0.71241819  0.05506528\n",
            "  0.24335074  0.06735023  1.07608612  0.35363084 -0.32317993 -0.01364578\n",
            " -1.23979493 -0.79509304 -0.28505602  0.12471863  0.35241554 -0.88394337\n",
            "  0.5313662   0.16861431  0.93480975  0.53212024 -0.11162097  0.30124419\n",
            "  0.07678222]\n",
            "Test data after feature standardization:  [-1.51078724e+00 -1.53831068e-01  1.20887672e-01 -5.90691693e-01\n",
            "  6.58759369e-02 -1.38307070e-01  1.70197615e-01  1.04953567e-03\n",
            "  1.90260981e-02  1.84219216e-01 -1.23460158e-01  2.34217226e-01\n",
            " -2.86858965e-01 -1.51196114e+00 -1.55800652e-01  1.23494463e-01\n",
            " -5.94077779e-01  6.43372517e-02 -1.38616349e-01  1.65808682e-01\n",
            "  1.46275192e-05  1.28237718e-02  1.85814597e-01 -1.21694827e-01\n",
            "  2.29655974e-01 -2.88030888e-01 -1.51190115e+00 -1.57019398e-01\n",
            "  1.22276207e-01 -5.95443807e-01  6.33968372e-02 -1.31285787e-01\n",
            "  1.66584504e-01 -2.19972341e-03  1.57287373e-02  1.81089478e-01\n",
            " -1.24071145e-01  2.33867022e-01 -2.83797992e-01 -1.51107177e+00\n",
            " -1.57752171e-01  1.13793351e-01 -5.95273740e-01  6.51500655e-02\n",
            " -1.38903835e-01  1.61553036e-01  5.22948272e-04  1.26345454e-02\n",
            "  1.74711645e-01 -1.33091455e-01  2.39064470e-01 -2.86650552e-01\n",
            " -1.51124005e+00 -1.58290725e-01  1.18357569e-01 -5.96696365e-01\n",
            "  6.14453975e-02 -1.34115170e-01  1.63151033e-01 -4.77289447e-03\n",
            "  1.38454982e-02  1.77930612e-01 -1.26387614e-01  2.33119193e-01\n",
            " -2.85059804e-01 -1.51103927e+00 -1.58507212e-01  1.15802248e-01\n",
            " -5.96599527e-01  6.04436317e-02 -1.44133467e-01  1.58983098e-01\n",
            " -5.33229240e-03  8.87897969e-03  1.79411893e-01 -1.26408391e-01\n",
            "  2.28330448e-01 -2.90360770e-01 -1.51005065e+00 -1.58150434e-01\n",
            "  1.09676968e-01 -5.94558423e-01  5.99500484e-02 -1.46595501e-01\n",
            "  1.60008440e-01 -7.23589549e-03  1.28992136e-02  1.74529302e-01\n",
            " -1.30751682e-01  2.32353812e-01 -2.90119642e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaGa18B100C",
        "colab_type": "text"
      },
      "source": [
        "Conversion of features to 'float32' type, and target arrays to Keras categorical format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A46_vg1X1LsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "da08fb03-656a-4882-d423-1737eebfa680"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "lmfcc_train_x = lmfcc_train_x.astype('float32')\n",
        "lmfcc_val_x = lmfcc_val_x.astype('float32')\n",
        "lmfcc_test_x = lmfcc_test_x.astype('float32')\n",
        "\n",
        "# train_y_test = list.copy(train_y)\n",
        "# for idx, state in enumerate(train_y):\n",
        "#   train_y_test[idx] = stateList.index(state)\n",
        "\n",
        "list_targets = [train_y, val_y, test_y]\n",
        "\n",
        "for dataset in list_targets:\n",
        "  for idx, state in enumerate(dataset):\n",
        "    dataset[idx] = stateList.index(state)\n",
        "\n",
        "train_y = np_utils.to_categorical(train_y, len(stateList))\n",
        "val_y = np_utils.to_categorical(val_y, len(stateList))\n",
        "test_y = np_utils.to_categorical(test_y, len(stateList))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cZv39GQ-0B8",
        "colab_type": "code",
        "outputId": "3baaac9e-1866-4365-def5-0c8307ae3927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(len(train_y))\n",
        "print(train_y[0])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1343422\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zydanardb6-m",
        "colab_type": "text"
      },
      "source": [
        "#5 Phoneme Recognition with Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjl-ePKUr2Bi",
        "colab_type": "text"
      },
      "source": [
        "Here are the minimum list of configurations to test, but you can test your favourite models if you manage to run the training in reasonable time. \n",
        "\n",
        "1. input: liftered MFCCs, one to four hidden layers of size 256, rectified linear units\n",
        "2. input: filterbank features, one to four hidden layers of size 256, rectified linear units\n",
        "3. same as 1. but with dynamic features as explained in Section 4.5\n",
        "4. same as 2. but with dynamic features as explained in Section 4.5\n",
        "\n",
        "Note the evolution of the loss function and the accuracy of the model for every epoch. What can you say comparing the results on the training and validation data? There are many other parameters that you can vary, if you have time to play with the models.\n",
        "For example:\n",
        "\n",
        "• different activation functions than ReLU\n",
        "\n",
        "• different number of hidden layers\n",
        "\n",
        "• different number of nodes per layer\n",
        "\n",
        "• different length of context input window\n",
        "\n",
        "• strategy to update learning rate and momentum\n",
        "\n",
        "• initialisation with DBNs instead of random\n",
        "\n",
        "• different normalisation of the feature vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv6Ki4JRcAn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-fDyq6cdh7h",
        "colab_type": "text"
      },
      "source": [
        "## lmfcc features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q71F3gqNdU7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2128b751-ee47-4c8b-c43b-3f61b812f901"
      },
      "source": [
        "lmfcc_train_x.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1343422, 91)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biykBz3Pc27E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "7787dc6f-0238-4b33-95f9-55103e72f323"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# A typical value for hidden nodes is 256, but you can experiment with this to see if convergence becomes faster or\n",
        "slower.\n",
        "model.add(Dense(64, input_shape=(91,), activation='relu', dtype=\"float32\"))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "model.add(Dense(61))\n",
        "model.add(Activation('softmax', name='posterior'))\n",
        "\n",
        "opt = Adam()\n",
        "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 64)                5888      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 61)                3965      \n",
            "_________________________________________________________________\n",
            "posterior (Activation)       (None, 61)                0         \n",
            "=================================================================\n",
            "Total params: 92,349\n",
            "Trainable params: 92,349\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwxmmvqheVg_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "dc95075a-bb5c-4769-a232-b916253a65b4"
      },
      "source": [
        "filepath = \"/content/drive/My Drive/Lab3/weights.{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,\n",
        "                             save_best_only=True, mode='max')\n",
        "\n",
        "model.fit(lmfcc_train_x, train_y, epochs=10, batch_size=64,\n",
        "          validation_data = (lmfcc_val_x, val_y), callbacks=checkpoint)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20991/20991 [==============================] - 62s 3ms/step - loss: 0.8520 - accuracy: 0.7214 - val_loss: 0.7253 - val_accuracy: 0.7632\n",
            "Epoch 2/10\n",
            "20991/20991 [==============================] - 62s 3ms/step - loss: 0.7031 - accuracy: 0.7652 - val_loss: 0.7050 - val_accuracy: 0.7687\n",
            "Epoch 3/10\n",
            "20991/20991 [==============================] - 61s 3ms/step - loss: 0.6722 - accuracy: 0.7748 - val_loss: 0.7142 - val_accuracy: 0.7668\n",
            "Epoch 4/10\n",
            "20991/20991 [==============================] - 63s 3ms/step - loss: 0.6543 - accuracy: 0.7805 - val_loss: 0.6984 - val_accuracy: 0.7721\n",
            "Epoch 5/10\n",
            "20991/20991 [==============================] - 61s 3ms/step - loss: 0.6426 - accuracy: 0.7839 - val_loss: 0.6914 - val_accuracy: 0.7747\n",
            "Epoch 6/10\n",
            "20991/20991 [==============================] - 61s 3ms/step - loss: 0.6338 - accuracy: 0.7864 - val_loss: 0.6950 - val_accuracy: 0.7770\n",
            "Epoch 7/10\n",
            "20991/20991 [==============================] - 62s 3ms/step - loss: 0.6273 - accuracy: 0.7883 - val_loss: 0.6871 - val_accuracy: 0.7784\n",
            "Epoch 8/10\n",
            "20991/20991 [==============================] - 60s 3ms/step - loss: 0.6220 - accuracy: 0.7900 - val_loss: 0.6885 - val_accuracy: 0.7775\n",
            "Epoch 9/10\n",
            "20991/20991 [==============================] - 62s 3ms/step - loss: 0.6176 - accuracy: 0.7917 - val_loss: 0.6933 - val_accuracy: 0.7810\n",
            "Epoch 10/10\n",
            "20991/20991 [==============================] - 60s 3ms/step - loss: 0.6143 - accuracy: 0.7929 - val_loss: 0.6911 - val_accuracy: 0.7784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fedaf123cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD305jQyfLaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "e2f81ed8-cbb6-4d08-e9da-9d6c88619839"
      },
      "source": [
        "print(\"Evaluating model on test data...\")\n",
        "results = model.evaluate(lmfcc_test_x, test_y)\n",
        "print(\"Test loss:\", results[0])\n",
        "print(\"Test acc:\", results[1])\n",
        "\n",
        "# IT NEEDS THE NON ONE HOT ENCODED HERE, WE SHOULD SAVE A COPY\n",
        "\"\"\"\n",
        "y_test_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(y_test, y_test_pred))\n",
        "print(\"Classification reprot:\")\n",
        "print(classification_report(y_test, y_test_pred, digits=8))\n",
        "\"\"\""
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating model on test data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-75bff901ac21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating model on test data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlmfcc_test_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test acc:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1055\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[1;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 163635\n  y sizes: 1526682\nPlease provide data which shares the same first dimension."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNuNHRwxih5M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f0ff9278-6042-442a-d317-3d7d13b121ba"
      },
      "source": [
        "print(lmfcc_test_x.shape)\n",
        "print(test_y.shape)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(163635, 91)\n",
            "(1526682, 61)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}